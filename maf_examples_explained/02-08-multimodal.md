# 02-08 â€” Multimodal Input

> **Source**: Provider-specific samples in [02-agents/providers/](https://github.com/microsoft/agent-framework/tree/main/python/samples/02-agents/providers) (e.g., `openai/`, `ollama/`)
> **Difficulty**: Intermediate
> **Prerequisites**: [01 â€” Get Started](01-get-started.md), [02-03 â€” Providers](02-03-providers.md)

## Overview

MAF supports **multimodal input** â€” sending images, audio, and files alongside text to agents. This is handled through the provider's native capabilities, wrapped in MAF's message system.

```mermaid
graph TB
    Input["User Input"]
    Input --> Text["Text"]
    Input --> Image["Image (URL/Base64)"]
    Input --> Audio["Audio"]
    Input --> File["File Attachments"]
    
    Text --> Agent["Agent"]
    Image --> Agent
    Audio --> Agent
    File --> Agent
```

---

## Sending Images

### URL-Based Image Input

```python
from agent_framework import Message, MessageContent

# Create a message with both text and an image URL
message = Message("user", [
    MessageContent.text("What's in this image?"),
    MessageContent.image_url("https://example.com/photo.jpg"),
])

result = await agent.run(message)
```

### Base64-Encoded Image

```python
import base64

with open("photo.png", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

message = Message("user", [
    MessageContent.text("Describe this image"),
    MessageContent.image_url(f"data:image/png;base64,{image_data}"),
])

result = await agent.run(message)
```

---

## Provider Support

| Provider | Images | Audio | Files | Notes |
|----------|--------|-------|-------|-------|
| OpenAI (GPT-4o) | âœ… | âœ… | âœ… | Full multimodal |
| Azure OpenAI | âœ… | âœ… | âœ… | Same as OpenAI |
| Anthropic (Claude) | âœ… | âŒ | âœ… | Vision only |
| Ollama (LLaVA) | âœ… | âŒ | âŒ | Vision models only |
| Amazon Bedrock | âœ… | âŒ | âŒ | Model dependent |

---

## ğŸ¯ Key Takeaways

1. **`MessageContent.image_url()`** â€” Pass images via URL or base64 data URIs
2. **Provider-dependent** â€” Not all models support all modalities
3. **Same agent API** â€” Multimodal input uses the same `agent.run()` interface
4. **Check provider samples** â€” See `providers/openai/` and `providers/ollama/` for working examples

## What's Next

â†’ [02-09 â€” Observability](02-09-observability.md) for tracing and monitoring
â†’ [02-10 â€” Streaming & Extras](02-10-streaming-and-extras.md) for response streaming
